m个training example，L是网络层数，sl是某一层的节点个数
正则项没有加入theta0，J是cost function，神经网络则对每一个输出节点合成了J
bp算法，求J对theta的偏导，前面的都是g(),最后一层是h(),delta表达的是每一层的error，（a-y），其中a就是h()，没有detla1
将差错传递回来，
theta的上标代表第几层，脚标代表是第几个神经元（j）到下一层的哪个神经元（i），表达式为ij，下一层在前，上一层在后
与前向传播的表达式很像
可以将logsitic造成的cost function类比于平方差
delta是cost关于z的偏导项
假如z有改变，那么会改变h(x),从而改变cost
因此detla由后面一层的delat乘以weight得到
【unrolling】将参数展开成向量，神经网络中的不是向量，
octave里面，这样就将theta1，2，3里面的元素全部取出来展开成一个长向量
将向量再重新展开为矩阵
因为写costfunction代码时，需要传入的是一个长向量
fminunc进行优化的时候需要将初始参数长向量传递进去
【gradient checking】保证求导正确
首先数值上估计导数，可以选取两边的两个点连线的斜率近似导数
当有多个theta，也就是将每个theta的求导单独考虑，epsilon相同，再对比bp算出的导数
【初始化参数】initialTheta，假如全部初始化为0，那么就只有bias在相加，那么会导致计算出的weight是相同的
上图相同颜色的线就一直相同，那么计算出来的特征就都是相同的，初始化的话需要相同大小的正负边界。
























































